{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=FutureWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import entropy\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# run \"pip install -U shorttext\" to install shorttext\n",
    "# to use shorttext, package \"en\" need to be downloaded using command \"python -m spacy download en\"\n",
    "from shorttext.utils import DocumentTermMatrix, standard_text_preprocessor_1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run entropy with base 2\n",
    "def entropy_2(p):\n",
    "    return entropy(p, base=2)\n",
    "\n",
    "# find out of the document is a Berardinelli document (name has 4 characters)\n",
    "def is_bern(doc_name: str):\n",
    "    return re.search('^\\d{4}\\.txt$', doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define system constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# working directory\n",
    "pwd: str = os.environ['HOME'] + '/work/assignment/assignment-7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus_root: str = pwd + '/MovieReviews'\n",
    "bern_file_count: int = 80\n",
    "sch_file_count: int = 100\n",
    "total_file_count: int = bern_file_count + sch_file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read all the files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Total files: 180\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "files: PlaintextCorpusReader = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "print('Total files:', len(files.fileids()))\n",
    "\n",
    "file_contents: list = [files.raw(file) for file in files.fileids()]\n",
    "print(len(file_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard_text_preprocessor_1 provides a standard way of text pre-processing:\n",
    "- removing special characters,\n",
    "- removing numerals,\n",
    "- converting all alphabets to lower cases,\n",
    "- removing stop words, and\n",
    "- stemming the words (using Porter stemmer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = standard_text_preprocessor_1()\n",
    "corpus: list = [preprocessor(sentence).split(' ') for sentence in file_contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the corpus to a document term matrix \n",
    "- each row represents a document\n",
    "- each column represents the number of occurrences for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtm: DocumentTermMatrix = DocumentTermMatrix(corpus, docids = files.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate Mutual Information for a given word against the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calc_mi(_word: str):\n",
    "    _word_docs: list = list(dtm.get_token_occurences(_word).keys())\n",
    "    _bern_doc_count: int = sum([1 for doc in _word_docs if is_bern(doc)])\n",
    "    _bern_no_doc_count: int = bern_file_count - _bern_doc_count\n",
    "    _sch_doc_count: int = sum([1 for doc in _word_docs if not is_bern(doc)])\n",
    "    _sch_no_doc_count: int = sch_file_count - _sch_doc_count\n",
    "    _matrix = np.reshape((_bern_doc_count, _bern_no_doc_count, _sch_doc_count, _sch_no_doc_count), (2,2))\n",
    "    _marginal_entropy = entropy_2(np.sum(_matrix, axis = 1))\n",
    "\n",
    "    _column_prob = np.sum(_matrix, axis = 0)/total_file_count\n",
    "    _column_entropy = np.apply_along_axis(entropy_2, 0, _matrix)\n",
    "    _conditional_entropy = sum(_column_prob * _column_entropy)\n",
    "    _mi = _marginal_entropy - _conditional_entropy\n",
    "    return _mi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get all distinct words from the corpus & calculate MI & store it in a vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Word Set: 10930\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "all_words: list = [word for review in corpus for word in review]\n",
    "distinct_words: set = set(all_words)\n",
    "print('Word Set:', len(distinct_words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Total MI: 10930\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "word_arr = []\n",
    "mi_arr = []\n",
    "\n",
    "for word in distinct_words:\n",
    "    mi = calc_mi(word)\n",
    "    word_arr.append(word)\n",
    "    mi_arr.append(mi)\n",
    "\n",
    "print('Total MI:', len(mi_arr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build a dataframe of the words with the MI value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(10930, 2)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "word_mi_tuples: list = list(zip(word_arr, mi_arr))\n",
    "word_mi_df: DataFrame = pd.DataFrame(word_mi_tuples, columns=['Word', 'MI'])\n",
    "print(word_mi_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sort by MI & extract the top 10 words having the highest MI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "             Word        MI\n9472     schwartz  0.991076\n1429       reserv  0.869352\n7268        denni  0.806275\n8900       review  0.643569\n4936        right  0.490622\n8778        howev  0.423902\n4556       releas  0.418957\n10116        cast  0.392004\n704    screenplay  0.385246\n7113       produc  0.372456\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "word_mi_df_sorted = word_mi_df.sort_values(by=['MI'], ascending=False)\n",
    "word_mi_df_top_10 = word_mi_df_sorted.head(10)\n",
    "print(word_mi_df_top_10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What it would mean for a word to have high MI with the document author?\n",
    "- Conditional entropy is used to identify syntagmatic relationship between words in a given context, in this case against the author.\n",
    "- Conditional entropy is absolute & cannot be used to compare across different words.\n",
    "- Mutual information helps normalize the conditional entropy value across words so it becomes comparable.\n",
    "- MI indicates a reduction in entropy, & a higher reduction in entropy indicates a strong relationship between the word & the author.\n",
    "- In the list of top 10 words, the words - schwartz, denni, reserv, review, right all appear with high MI, since all the reviews by Schwartz have the footer text in the reviews with the copy right notice.\n",
    "- So, the appearance of those words will have a strong indication that the review's author was Schwartz.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
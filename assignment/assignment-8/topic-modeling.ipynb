{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "np.random.seed(2018)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# working directory\n",
    "pwd: str = os.environ['HOME'] + '/work/assignment/assignment-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stemmer: PorterStemmer = PorterStemmer()\n",
    "lemmatizer: WordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(lemmatizer.lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the text files in the directory\n",
    "- get the filenames list\n",
    "- loop through the filenames & read each file\n",
    "- add the filename & the file content into a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path: str = pwd + '/MovieReviews'\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "doc_contents: list = []\n",
    "for i, file_name in zip(range(len(file_names)), file_names):\n",
    "    with open(path + '/' + file_name, encoding=\"utf8\", errors='ignore') as file:\n",
    "        doc_contents.append((i, file_name, file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the document contents into a dataframe\n",
    "- load the tuple into a dataframe\n",
    "- remove any NA values in the 'FileContent' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNum   FileName                                        FileContent\n",
      "0       0  16748.txt  DENNIS SCHWARTZ \"Movie Reviews and Poetry\"\\nUN...\n",
      "1       1  17108.txt  A brilliant, witty mock documentary of Jean Se...\n",
      "2       2  17109.txt  NOSTALGHIA (director: Andrei Tarkovsky; cast: ...\n",
      "3       3  17110.txt  PAYBACK (director: Brian Helgeland; cast:(Port...\n",
      "4       4  17111.txt  WAKING NED DEVINE (director: Kirk Jones (III);...\n"
     ]
    }
   ],
   "source": [
    "data: DataFrame = pd.DataFrame(doc_contents, columns=['RowNum', 'FileName', 'FileContent'])\n",
    "data.dropna(subset=['FileContent'], inplace= True)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dictionary out of the document contents\n",
    "- execute lemmatization & stemming actions for each item in the dataframe\n",
    "- build dictionary object containing the number of times a word appears in the document set.\n",
    "- Filter out tokens that appear in \n",
    "    - less than 15 documents (absolute number) or \n",
    "    - more than 0.5 documents (fraction of total corpus size, not absolute number)\n",
    "    - keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [denni, schwartz, movi, review, poetri, unmak,...\n",
      "1    [brilliant, witti, mock, documentari, jean, se...\n",
      "2    [nostalghia, director, andrei, tarkovski, cast...\n",
      "3    [payback, director, brian, helgeland, cast, po...\n",
      "4    [wake, devin, director, kirk, jone, cast, bann...\n",
      "Name: FileContent, dtype: object\n"
     ]
    }
   ],
   "source": [
    "processed_docs = data['FileContent'].map(preprocess)\n",
    "print(processed_docs[:5])\n",
    "\n",
    "dictionary: Dictionary = Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build BagOfWords corpus from the document content\n",
    "- for each document, create a dictionary reporting how many words and how many times they appear.\n",
    "- build dictionary object containing the number of times a word appears in the document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out one of the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"accomplish\") appears 1 times.\n",
      "Word 1 (\"actor\") appears 1 times.\n",
      "Word 2 (\"america\") appears 1 times.\n",
      "Word 3 (\"apart\") appears 1 times.\n",
      "Word 4 (\"attract\") appears 3 times.\n"
     ]
    }
   ],
   "source": [
    "test_corpus = bow_corpus[0]\n",
    "for i, word_stat in zip(range(len(test_corpus)), test_corpus):\n",
    "    # print only the first 5 words\n",
    "    if i < 5:\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} times.\".format(word_stat[0], dictionary[word_stat[0]], word_stat[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LDA model\n",
    "- num_topics value is set as 2, indicating to get the top 2 k=2 models.\n",
    "- for each topic, explore the words occurring in that topic and its relative weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 -> Words: 0.008*\"littl\" + 0.006*\"money\" + 0.005*\"audienc\" + 0.005*\"father\" + 0.005*\"role\" + 0.005*\"show\" + 0.005*\"perform\" + 0.005*\"feel\" + 0.005*\"releas\" + 0.005*\"real\"\n",
      "Topic: 1 -> Words: 0.008*\"world\" + 0.007*\"feel\" + 0.007*\"see\" + 0.006*\"need\" + 0.006*\"tri\" + 0.005*\"question\" + 0.005*\"turn\" + 0.005*\"wife\" + 0.005*\"say\" + 0.005*\"real\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaMulticore(bow_corpus, num_topics=2, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} -> Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words for each of the topics\n",
    "- identify the top 10 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 -> Top Words: [(0.008109855, 'littl'), (0.0057991627, 'money'), (0.0053513288, 'audienc'), (0.0053070267, 'father'), (0.0052273963, 'role'), (0.005220431, 'show'), (0.005144959, 'perform'), (0.005120052, 'feel'), (0.0050592427, 'releas'), (0.004968191, 'real')]\n",
      "Topic: 1 -> Top Words: [(0.00778954, 'world'), (0.007067585, 'feel'), (0.006778942, 'see'), (0.006349574, 'need'), (0.0060187713, 'tri'), (0.0051947045, 'question'), (0.0049245534, 'turn'), (0.0049069873, 'wife'), (0.004820435, 'say'), (0.00475309, 'real')]\n"
     ]
    }
   ],
   "source": [
    "top_topics = lda_model.top_topics(corpus=bow_corpus, topn=10)\n",
    "\n",
    "i = 0\n",
    "for words, coherence in top_topics:\n",
    "    print('Topic: {} -> Top Words: {}'.format(i, words))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document -> Topic probabilities for all the documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 16748.txt -> [(0, 0.13139327), (1, 0.8686067)]\n",
      "1 - 17108.txt -> [(0, 0.71595013), (1, 0.2840499)]\n",
      "2 - 17109.txt -> [(1, 0.99555296)]\n",
      "3 - 17110.txt -> [(0, 0.9443173), (1, 0.055682763)]\n",
      "4 - 17111.txt -> [(0, 0.5102283), (1, 0.48977178)]\n",
      "5 - 17116.txt -> [(0, 0.09412225), (1, 0.90587777)]\n",
      "6 - 17117.txt -> [(0, 0.20857136), (1, 0.7914286)]\n",
      "7 - 17118.txt -> [(1, 0.99045455)]\n",
      "8 - 17119.txt -> [(0, 0.06693634), (1, 0.9330637)]\n",
      "9 - 17139.txt -> [(1, 0.99297714)]\n",
      "10 - 17144.txt -> [(0, 0.58954734), (1, 0.41045266)]\n",
      "11 - 17145.txt -> [(0, 0.010423786), (1, 0.9895762)]\n",
      "12 - 17146.txt -> [(0, 0.34672955), (1, 0.6532704)]\n",
      "13 - 17147.txt -> [(0, 0.29252928), (1, 0.7074707)]\n",
      "14 - 17150.txt -> [(0, 0.014667761), (1, 0.98533225)]\n",
      "15 - 17185.txt -> [(0, 0.9744871), (1, 0.025512876)]\n",
      "16 - 17192.txt -> [(0, 0.25499335), (1, 0.7450067)]\n",
      "17 - 17219.txt -> [(0, 0.9094785), (1, 0.09052149)]\n",
      "18 - 17239.txt -> [(0, 0.032951564), (1, 0.96704847)]\n",
      "19 - 17243.txt -> [(0, 0.80417323), (1, 0.19582674)]\n",
      "20 - 17254.txt -> [(0, 0.0126566775), (1, 0.9873433)]\n",
      "21 - 17255.txt -> [(0, 0.5776561), (1, 0.4223439)]\n",
      "22 - 17280.txt -> [(0, 0.6486904), (1, 0.3513096)]\n",
      "23 - 17300.txt -> [(0, 0.81967425), (1, 0.18032579)]\n",
      "24 - 17303.txt -> [(0, 0.1572277), (1, 0.8427723)]\n",
      "25 - 17341.txt -> [(0, 0.031123618), (1, 0.9688764)]\n",
      "26 - 17384.txt -> [(0, 0.26351172), (1, 0.7364883)]\n",
      "27 - 17391.txt -> [(0, 0.9868448), (1, 0.013155239)]\n",
      "28 - 17398.txt -> [(1, 0.99287254)]\n",
      "29 - 17399.txt -> [(0, 0.06951611), (1, 0.9304839)]\n",
      "30 - 17430.txt -> [(0, 0.89940983), (1, 0.10059016)]\n",
      "31 - 17431.txt -> [(0, 0.07887486), (1, 0.9211252)]\n",
      "32 - 17447.txt -> [(0, 0.010706374), (1, 0.98929363)]\n",
      "33 - 17457.txt -> [(0, 0.52313626), (1, 0.4768637)]\n",
      "34 - 17460.txt -> [(0, 0.2378755), (1, 0.7621245)]\n",
      "35 - 17501.txt -> [(0, 0.05168529), (1, 0.9483147)]\n",
      "36 - 17518.txt -> [(0, 0.53847754), (1, 0.46152246)]\n",
      "37 - 17532.txt -> [(0, 0.15466765), (1, 0.8453324)]\n",
      "38 - 17534.txt -> [(0, 0.9321086), (1, 0.06789141)]\n",
      "39 - 17578.txt -> [(0, 0.39045775), (1, 0.6095422)]\n",
      "40 - 17609.txt -> [(0, 0.6681685), (1, 0.33183149)]\n",
      "41 - 17610.txt -> [(0, 0.53348), (1, 0.46651998)]\n",
      "42 - 17655.txt -> [(0, 0.04545451), (1, 0.95454544)]\n",
      "43 - 17662.txt -> [(1, 0.99100363)]\n",
      "44 - 17663.txt -> [(0, 0.34588367), (1, 0.6541164)]\n",
      "45 - 17695.txt -> [(0, 0.73531806), (1, 0.26468194)]\n",
      "46 - 17711.txt -> [(0, 0.9780072), (1, 0.021992799)]\n",
      "47 - 17713.txt -> [(0, 0.789824), (1, 0.21017599)]\n",
      "48 - 17753.txt -> [(0, 0.9793207), (1, 0.020679343)]\n",
      "49 - 17757.txt -> [(0, 0.20616941), (1, 0.7938305)]\n",
      "50 - 17758.txt -> [(0, 0.14206067), (1, 0.85793936)]\n",
      "51 - 17761.txt -> [(0, 0.9526283), (1, 0.04737168)]\n",
      "52 - 17803.txt -> [(0, 0.056536008), (1, 0.94346404)]\n",
      "53 - 17811.txt -> [(0, 0.1377117), (1, 0.8622883)]\n",
      "54 - 17874.txt -> [(0, 0.41230932), (1, 0.58769065)]\n",
      "55 - 17879.txt -> [(0, 0.9544103), (1, 0.04558973)]\n",
      "56 - 17886.txt -> [(0, 0.684698), (1, 0.31530207)]\n",
      "57 - 17896.txt -> [(0, 0.19316833), (1, 0.8068317)]\n",
      "58 - 17898.txt -> [(1, 0.99470854)]\n",
      "59 - 17902.txt -> [(1, 0.99470705)]\n",
      "60 - 17912.txt -> [(0, 0.28643307), (1, 0.71356696)]\n",
      "61 - 17933.txt -> [(0, 0.12923056), (1, 0.8707694)]\n",
      "62 - 17934.txt -> [(0, 0.16475971), (1, 0.83524024)]\n",
      "63 - 17945.txt -> [(0, 0.88401294), (1, 0.11598709)]\n",
      "64 - 17963.txt -> [(0, 0.015035958), (1, 0.9849641)]\n",
      "65 - 17971.txt -> [(0, 0.3528243), (1, 0.6471757)]\n",
      "66 - 17992.txt -> [(0, 0.7556586), (1, 0.24434139)]\n",
      "67 - 18004.txt -> [(0, 0.9643048), (1, 0.03569516)]\n",
      "68 - 18016.txt -> [(0, 0.09268329), (1, 0.9073167)]\n",
      "69 - 18032.txt -> [(0, 0.67802), (1, 0.32197997)]\n",
      "70 - 18067.txt -> [(0, 0.7007251), (1, 0.29927492)]\n",
      "71 - 18068.txt -> [(0, 0.527337), (1, 0.47266302)]\n",
      "72 - 18080.txt -> [(0, 0.025857178), (1, 0.9741428)]\n",
      "73 - 18087.txt -> [(0, 0.036457874), (1, 0.9635421)]\n",
      "74 - 18088.txt -> [(0, 0.016144544), (1, 0.9838554)]\n",
      "75 - 18136.txt -> [(0, 0.9056814), (1, 0.09431864)]\n",
      "76 - 18141.txt -> [(0, 0.2439941), (1, 0.75600594)]\n",
      "77 - 18156.txt -> [(0, 0.012748227), (1, 0.98725176)]\n",
      "78 - 18161.txt -> [(0, 0.014475851), (1, 0.9855242)]\n",
      "79 - 18181.txt -> [(0, 0.86485994), (1, 0.13514003)]\n",
      "80 - 18227.txt -> [(0, 0.28555113), (1, 0.7144489)]\n",
      "81 - 18263.txt -> [(0, 0.0142764), (1, 0.98572356)]\n",
      "82 - 18272.txt -> [(0, 0.1456693), (1, 0.8543308)]\n",
      "83 - 18273.txt -> [(0, 0.06771171), (1, 0.9322882)]\n",
      "84 - 18274.txt -> [(0, 0.09061583), (1, 0.9093842)]\n",
      "85 - 18282.txt -> [(0, 0.25089604), (1, 0.74910396)]\n",
      "86 - 18283.txt -> [(0, 0.5635622), (1, 0.43643782)]\n",
      "87 - 18307.txt -> [(1, 0.9960255)]\n",
      "88 - 18368.txt -> [(0, 0.10549232), (1, 0.89450765)]\n",
      "89 - 18375.txt -> [(1, 0.99063003)]\n",
      "90 - 18376.txt -> [(1, 0.99680877)]\n",
      "91 - 18396.txt -> [(0, 0.20214267), (1, 0.7978574)]\n",
      "92 - 18406.txt -> [(0, 0.617239), (1, 0.38276097)]\n",
      "93 - 18413.txt -> [(0, 0.17984794), (1, 0.82015204)]\n",
      "94 - 18414.txt -> [(0, 0.14062811), (1, 0.8593719)]\n",
      "95 - 18447.txt -> [(0, 0.84865206), (1, 0.15134797)]\n",
      "96 - 18473.txt -> [(0, 0.8492903), (1, 0.15070975)]\n",
      "97 - 18480.txt -> [(0, 0.29215032), (1, 0.7078497)]\n",
      "98 - 18485.txt -> [(0, 0.356537), (1, 0.643463)]\n",
      "99 - 18498.txt -> [(0, 0.10288964), (1, 0.8971103)]\n",
      "100 - 1858.txt -> [(0, 0.9853107), (1, 0.014689284)]\n",
      "101 - 1859.txt -> [(0, 0.7237614), (1, 0.27623862)]\n",
      "102 - 1860.txt -> [(0, 0.7860445), (1, 0.21395549)]\n",
      "103 - 1864.txt -> [(0, 0.9819358), (1, 0.018064227)]\n",
      "104 - 1865.txt -> [(0, 0.9309065), (1, 0.069093496)]\n",
      "105 - 1866.txt -> [(0, 0.9933921)]\n",
      "106 - 1867.txt -> [(0, 0.9149654), (1, 0.08503459)]\n",
      "107 - 1889.txt -> [(0, 0.9804449), (1, 0.019555088)]\n",
      "108 - 1891.txt -> [(0, 0.9721377), (1, 0.027862316)]\n",
      "109 - 1908.txt -> [(0, 0.98325944), (1, 0.016740596)]\n",
      "110 - 1910.txt -> [(0, 0.96329314), (1, 0.036706813)]\n",
      "111 - 1911.txt -> [(0, 0.991127)]\n",
      "112 - 1912.txt -> [(0, 0.9841058), (1, 0.015894158)]\n",
      "113 - 1916.txt -> [(0, 0.8543625), (1, 0.14563751)]\n",
      "114 - 1917.txt -> [(0, 0.93851936), (1, 0.061480656)]\n",
      "115 - 1921.txt -> [(0, 0.99064094)]\n",
      "116 - 1925.txt -> [(0, 0.9905114)]\n",
      "117 - 1928.txt -> [(0, 0.8674685), (1, 0.1325315)]\n",
      "118 - 1929.txt -> [(0, 0.91459966), (1, 0.085400335)]\n",
      "119 - 1930.txt -> [(0, 0.9929352)]\n",
      "120 - 1932.txt -> [(0, 0.9429719), (1, 0.057028074)]\n",
      "121 - 1934.txt -> [(0, 0.99213994)]\n",
      "122 - 1937.txt -> [(0, 0.9544058), (1, 0.04559419)]\n",
      "123 - 1943.txt -> [(0, 0.8766221), (1, 0.12337794)]\n",
      "124 - 1944.txt -> [(0, 0.95680106), (1, 0.04319892)]\n",
      "125 - 1945.txt -> [(0, 0.9015046), (1, 0.09849544)]\n",
      "126 - 1961.txt -> [(0, 0.9909494)]\n",
      "127 - 1967.txt -> [(0, 0.95258594), (1, 0.04741405)]\n",
      "128 - 1968.txt -> [(0, 0.992192)]\n",
      "129 - 1974.txt -> [(0, 0.99356395)]\n",
      "130 - 1975.txt -> [(0, 0.97109956), (1, 0.028900439)]\n",
      "131 - 1976.txt -> [(0, 0.8271212), (1, 0.17287883)]\n",
      "132 - 1979.txt -> [(0, 0.9767335), (1, 0.023266492)]\n",
      "133 - 1981.txt -> [(0, 0.9956778)]\n",
      "134 - 1984.txt -> [(0, 0.9880464), (1, 0.011953614)]\n",
      "135 - 1985.txt -> [(0, 0.9834331), (1, 0.016566878)]\n",
      "136 - 1990.txt -> [(0, 0.98559386), (1, 0.014406146)]\n",
      "137 - 1991.txt -> [(0, 0.4921947), (1, 0.5078052)]\n",
      "138 - 1994.txt -> [(0, 0.9826388), (1, 0.017361285)]\n",
      "139 - 1998.txt -> [(0, 0.9728777), (1, 0.027122302)]\n",
      "140 - 2005.txt -> [(0, 0.9831205), (1, 0.016879464)]\n",
      "141 - 2006.txt -> [(0, 0.8765895), (1, 0.12341049)]\n",
      "142 - 2007.txt -> [(0, 0.99571496)]\n",
      "143 - 2008.txt -> [(0, 0.9801571), (1, 0.019842956)]\n",
      "144 - 2009.txt -> [(0, 0.7848468), (1, 0.21515319)]\n",
      "145 - 2025.txt -> [(0, 0.84609133), (1, 0.15390868)]\n",
      "146 - 2026.txt -> [(0, 0.9884044), (1, 0.011595648)]\n",
      "147 - 2030.txt -> [(0, 0.99374974)]\n",
      "148 - 2031.txt -> [(0, 0.49810335), (1, 0.5018967)]\n",
      "149 - 2033.txt -> [(0, 0.9639254), (1, 0.03607464)]\n",
      "150 - 2035.txt -> [(0, 0.96543574), (1, 0.034564294)]\n",
      "151 - 2036.txt -> [(0, 0.26973253), (1, 0.73026747)]\n",
      "152 - 2043.txt -> [(0, 0.58181286), (1, 0.41818714)]\n",
      "153 - 2045.txt -> [(0, 0.9910794)]\n",
      "154 - 2046.txt -> [(0, 0.62668633), (1, 0.37331367)]\n",
      "155 - 2051.txt -> [(0, 0.6605429), (1, 0.33945706)]\n",
      "156 - 2055.txt -> [(0, 0.95054376), (1, 0.04945627)]\n",
      "157 - 2058.txt -> [(0, 0.98380935), (1, 0.016190624)]\n",
      "158 - 2059.txt -> [(0, 0.8912381), (1, 0.10876189)]\n",
      "159 - 2062.txt -> [(0, 0.9580591), (1, 0.041940957)]\n",
      "160 - 2067.txt -> [(0, 0.97840744), (1, 0.021592531)]\n",
      "161 - 2076.txt -> [(0, 0.033173677), (1, 0.9668264)]\n",
      "162 - 2079.txt -> [(0, 0.92199194), (1, 0.078008056)]\n",
      "163 - 2080.txt -> [(0, 0.9337695), (1, 0.066230446)]\n",
      "164 - 2081.txt -> [(0, 0.9762982), (1, 0.023701804)]\n",
      "165 - 2085.txt -> [(0, 0.47180313), (1, 0.52819693)]\n",
      "166 - 2086.txt -> [(0, 0.97892034), (1, 0.021079639)]\n",
      "167 - 2087.txt -> [(0, 0.99202335)]\n",
      "168 - 2089.txt -> [(0, 0.99147755)]\n",
      "169 - 2090.txt -> [(0, 0.4481396), (1, 0.5518604)]\n",
      "170 - 2091.txt -> [(0, 0.5584312), (1, 0.4415688)]\n",
      "171 - 2094.txt -> [(0, 0.9940879)]\n",
      "172 - 2095.txt -> [(0, 0.527124), (1, 0.47287595)]\n",
      "173 - 2098.txt -> [(0, 0.64723676), (1, 0.35276327)]\n",
      "174 - 2099.txt -> [(0, 0.7438716), (1, 0.2561283)]\n",
      "175 - 2101.txt -> [(0, 0.13309342), (1, 0.8669066)]\n",
      "176 - 2108.txt -> [(0, 0.992939)]\n",
      "177 - 2110.txt -> [(0, 0.99437106)]\n",
      "178 - 2113.txt -> [(0, 0.93824136), (1, 0.061758686)]\n",
      "179 - 2115.txt -> [(0, 0.90316683), (1, 0.09683314)]\n"
     ]
    }
   ],
   "source": [
    "for i, corpus_item in zip(range(len(bow_corpus)), bow_corpus):\n",
    "    print(data['RowNum'][i], '-', data['FileName'][i], '->', lda_model[corpus_item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

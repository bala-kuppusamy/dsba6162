{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find out of the document is a Berardinelli document (name has 4 characters); \n",
    "# -> returns the author name\n",
    "def is_bern(doc_name: str) -> str: \n",
    "    match = re.search('^\\d{4}\\.txt$', doc_name)\n",
    "    return 'Berardinelli' if match is not None else 'Schwartz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define system constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# working directory\n",
    "pwd: str = os.environ['HOME'] + '/work/assignment/assignment-7'\n",
    "corpus_root: str = pwd + '/MovieReviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of files: 180\n",
      "File contents: 180\n",
      "Count of Author names: 180\n"
     ]
    }
   ],
   "source": [
    "files: PlaintextCorpusReader = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n",
    "print('Count of files:', len(files.fileids()))\n",
    "\n",
    "file_contents: list = [files.raw(file) for file in files.fileids()]\n",
    "print('File contents:', len(file_contents))\n",
    "\n",
    "authors: list = [is_bern(fileId) for fileId in files.fileids()]\n",
    "print('Count of Author names:', len(authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the corpus\n",
    "- This is to vectorize the text corpus. After these codes, the X object will be the input vector for machine learning models.\n",
    "- When transform into vectors, we do NOT use the raw count of a word in a document. Instead, we use the word's tf-idf score in a document.\n",
    "- max_df=0.5 means ignoring words that appear in more than 50% of the documents; min_df=2 means ignoring words that appear in less than 2 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dimension (before dim reduction): (180, 6704)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(file_contents)\n",
    "print('Original dimension (before dim reduction):', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the corpus\n",
    "- Use SVD (Singular Value Decomposition), a common matrix decomposition technique to reduce the dimensionality.\n",
    "- We have to re-normalize after we run our SVD on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimension (after dim reduction): (180, 3)\n"
     ]
    }
   ],
   "source": [
    "n_components = 3\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X_reduced = lsa.fit_transform(X)\n",
    "print('Reduced dimension (after dim reduction):', X_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select training data set (random 80% data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set: (144, 3)\n"
     ]
    }
   ],
   "source": [
    "training_size: int = math.ceil(len(X_reduced) * 0.8)\n",
    "\n",
    "training_idx = np.random.choice(X_reduced.shape[0], size=training_size, replace=False)\n",
    "X_training = X_reduced[training_idx, :]\n",
    "print('Training data set:', X_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select test data set (remaining 20% data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set: (36, 3)\n"
     ]
    }
   ],
   "source": [
    "test_idx = list(set(range(X_reduced.shape[0])) - set(training_idx))\n",
    "X_test = X_reduced[test_idx, :]\n",
    "print('Test data set:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the label into training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label test data set: ['Schwartz', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Schwartz', 'Schwartz', 'Schwartz', 'Berardinelli', 'Schwartz', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Schwartz', 'Berardinelli', 'Schwartz', 'Berardinelli', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli']\n"
     ]
    }
   ],
   "source": [
    "labels_training = [authors[i] for i in training_idx]\n",
    "labels_test = [authors[j] for j in test_idx]\n",
    "print('Label test data set:', labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Naive Bayes Classifier\n",
    "- Create the classifier\n",
    "- Fit the classifier with training data & labels\n",
    "- Predict the labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Schwartz', 'Berardinelli', 'Berardinelli', 'Berardinelli',\n",
       "       'Berardinelli', 'Schwartz', 'Schwartz', 'Schwartz', 'Berardinelli',\n",
       "       'Schwartz', 'Berardinelli', 'Berardinelli', 'Berardinelli',\n",
       "       'Berardinelli', 'Berardinelli', 'Schwartz', 'Berardinelli',\n",
       "       'Schwartz', 'Berardinelli', 'Schwartz', 'Schwartz', 'Schwartz',\n",
       "       'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz', 'Schwartz',\n",
       "       'Schwartz', 'Schwartz', 'Schwartz', 'Berardinelli', 'Berardinelli',\n",
       "       'Berardinelli', 'Berardinelli', 'Berardinelli', 'Berardinelli'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_training, labels_training)\n",
    "gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction metrics\n",
    "- Calculate Confusion Matrix\n",
    "- Calculate Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[18  0]\n",
      " [ 0 18]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix:', confusion_matrix(gnb.predict(X_test),labels_test))\n",
    "\n",
    "print('Accuracy:', accuracy_score(gnb.predict(X_test),labels_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
